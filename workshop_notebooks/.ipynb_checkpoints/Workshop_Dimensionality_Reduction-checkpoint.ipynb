{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "- A data transformation technique converts high-dimensional data to lower-dimensional representation\n",
    "- The idea is to removed unneccessary information/ features which includes the noise from the input so the model can learn more useful information from important features\n",
    "- It helps to find another basis or `principal components` to help classify the data better. It is similar to changing the perspective to look at the feature space at a different angle and find the best angle to observe all the variance/ spread in the data\n",
    "- Data needs to be scaled for PCA to work\n",
    "- PCA is useful for:\n",
    "    * Identify hidden (latent) useful features in high dimensional dataset\n",
    "    * Can be used for data compression, visualisation\n",
    "    * Speed up training and avoid overfitting the data\n",
    "    * It was proposed in 1901!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset with large amount of features has the problem which is known as the Curse of Dimensionality. Humans have a hard time visualise or think about high dimensional space. We are so used to 1D, 2D or 3D visualisation. In large dimensional space (100 or 1000 dimensions), things behave quite different with what we know. In high dimensions, the datapoints lies far away from each other. The sparsity of datapoints makes it extremely hard to do prediction or classification since most algorithms rely on points being close together to do extrapolation. In other to compensate for this sparsity, we need exponentially more data to populate the features space. \n",
    "There exists a lower k-dimensional space which can be used to describe the original d-dimensional data. \n",
    "\n",
    "<center><img src='./assets/pca.png' width=\"500\"></center>\n",
    "(Adapted from \"Hands on Machine Learning with scikit-learn 2ed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start off by building a 3-D toy dataset, then uses PCA to transform it into 2D dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the 3D dataset\n",
    "np.random.seed(4)\n",
    "m = 60\n",
    "w1, w2 = 0.1, 0.3\n",
    "noise = 0.1\n",
    "\n",
    "angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n",
    "X = np.empty((m, 3))\n",
    "X[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2\n",
    "X[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2\n",
    "X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(X[:,0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the original 3D dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "\n",
    "ax.plot(X[:, 0], X[:, 1], X[:, 2], \"bo\", alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('X1 Label')\n",
    "ax.set_ylabel('X2 Label')\n",
    "ax.set_zlabel('X3 Label')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventhough the data is three dimensional. Most of them can be approximately projected onto a 2D plane. The red and green vectors are the basis of this plane which we will find using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3D(X, figsize=(12,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how to use PCA in scikit-learn to reduce the 3D dataset into a new format with 2 dimensions. \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# set explicitly that we want 2 transformed features or 2 principal components\n",
    "# TODO:\n",
    "# Your code goes here\n",
    "\n",
    "\n",
    "\n",
    "# use this to inverse the process if we want to reconstruct the original data from the principal components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main outputs from performing a PCA on a dataset. Firstly it is the list of principal components, and secondly it is how much variance in the original data can be explained by each component. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill in the found principal component vectors\n",
    "# Your code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill in the explained variance\n",
    "# Your code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be interpreted as 84.25% of the total variance in the original dataset can be captured if we project the data onto the 1st component. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill in the explained variance\n",
    "# Your code goes here\n",
    "\n",
    "# 1st Principal component (red color)\n",
    "print('First component:'  )\n",
    "# 2nd Principal component (green color)\n",
    "print('Second component:'  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is the projection of the 2D data onto the two new axises Z1 and Z2. As you can see, Z1 and Z2 are the new basis that best describe the variance/ spread of our toy data. The use of z1 and z2 as input to the ML model would be much better and useful than the original axises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, aspect='equal')\n",
    "\n",
    "ax.plot(X2D[:, 0], X2D[:, 1], \"k+\")\n",
    "ax.plot(X2D[:, 0], X2D[:, 1], \"k.\")\n",
    "ax.plot([0], [0], \"ko\")\n",
    "ax.arrow(0, 0, 0, 1, head_width=0.05, length_includes_head=True, head_length=0.1, fc='k', ec='k')\n",
    "ax.arrow(0, 0, 1, 0, head_width=0.05, length_includes_head=True, head_length=0.1, fc='k', ec='k')\n",
    "ax.set_xlabel(\"$z_1$\", fontsize=18)\n",
    "ax.set_ylabel(\"$z_2$\", fontsize=18, rotation=0)\n",
    "ax.axis([-1.5, 1.3, -1.2, 1.2])\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply PCA on hand-written digits dataset:\n",
    "- This dataset has 784 features in total. Where each feature is the value of a single pixel. The original image is 2D array with the shape is 28x28. We need to flatten it out to 1D array of 784 values to fit it with a machine learning model. \n",
    "- If we examine some images of this dataset, it is easy to see that not all 784 pixels per image are valuable, just the ones in the center actually tell which digit it is. That is what we are trying to figure out using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset using scikit-learn function\n",
    "mnist_dataset = fetch_openml('mnist_784', version=1, cache=False)\n",
    "mnist_dataset.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the input features X and the target label Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist_dataset['data'], mnist_dataset['target']\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An image is a 2-dimensional array of numbers where each of the number represents the \"darkness\" of the pixel. A value of zero indicates a white pixel and a value of 255 represents a dark pixel. \n",
    "Each image in MNIST was flattened out into a vector of size (784,). So in order to view the image in its original form, we need to reshape it back into a 2D matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Explore a few digits from the MNIST dataset\n",
    "# Your code goes here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_vector = X[0]\n",
    "digit_image = image_vector.reshape(28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the image to see the digit, this looks like a 5\n",
    "plt.imshow(digit_image, cmap=mpl.cm.binary)\n",
    "plt.show()\n",
    "\n",
    "# let's print out the actual label of this digit\n",
    "print('Label for X[0]:',y[0]) \n",
    "# since y[0] is of type string, we need to convert into integers\n",
    "y = y.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to display an image of the digit so we can reuse it later on\n",
    "def plot_digit(img_vector):\n",
    "    digit_image = img_vector.reshape(28, 28)\n",
    "    plt.imshow(digit_image, cmap=mpl.cm.binary)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first 5 vector does include a variety of digits, so we don't have to shuffle them ourselves\n",
    "for i in range(5):\n",
    "    plot_digit(X[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Use train_test_split to have training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the PCA module, without configuring the number of components\n",
    "# we are going to figure out the number of components in order to maintain 95% the original total variance\n",
    "# TODO:\n",
    "# Your code goes here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the cumulative sum for all the principal components\n",
    "\n",
    "\n",
    "\n",
    "# pick d components where cumsum >= 0.95\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 154 latent features which we can use to train a model instead of 784 original features! And it can describe 95% the total variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cumsum(cumsum, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the PCA again this time with the found number of components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Your code goes here:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the dimensions of pca components\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curious to see the found components ourself?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 8, figsize=(9, 4),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(pca.components_[i].reshape(28, 28), cmap=mpl.cm.binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One cool feature is that you can inverse the process to reconstruct the original image from only 154 components. Any applications you can think of?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_recovered = pca.inverse_transform(X_reduced)\n",
    "print(X_recovered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    plot_digit(X_recovered[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 154)\n",
    "X_reduced = pca.fit_transform(X_train)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "log_clf = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", max_iter=2000, random_state=42)\n",
    "t0 = time.time()\n",
    "log_clf.fit(X_train, y_train)\n",
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training took {:.2f}s\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = log_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply PCA on both train and test data and train again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reduced = pca.transform(X_train)\n",
    "X_test_reduced = pca.transform(X_test)\n",
    "\n",
    "log_clf_pca = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\",max_iter=2000, random_state=42)\n",
    "t0 = time.time()\n",
    "log_clf_pca.fit(X_train_reduced, y_train)\n",
    "t1 = time.time()\n",
    "\n",
    "print(\"Training took {:.2f}s\".format(t1 - t0))\n",
    "y_pred = log_clf_pca.predict(X_test_reduced)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The training is much faster however it comes with the cost of a degraded accuracy. \n",
    "- PCA is not the main tool used to improve accuracy. Its purpose is stated at the beginning and it is up to us to see how to leverage those informations for model interpretability, compression as well as visualisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
