{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree algorithm\n",
    "- Used for both classification and regression\n",
    "- Non-parametric model, which means the number of parameter is not fixed prior to the training, which also means\n",
    "it has the risk of becoming overfitting as it tries to follow closely with the training data\n",
    "- Easy to interpret the decision of the model\n",
    "- One famous example of Decision Tree illustration:\n",
    "<center><img src='./assets/decision_tree_obama.jpg' width=\"600\" height=\"800\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Classification\n",
    "- To explain the working mechanism of DecisionTreeClassifier, let's have a look at the classification of the infamous iris dataset\n",
    "which come with scikitlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "from utils import plot_decision_boundary, plot_2d_boundary\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\")\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "# Load and inspect the iris dataset\n",
    "# TODO: \n",
    "# Your code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DecisionTreeClassifier and fit the iris dataset\n",
    "# TODO: use 2 feature petals length and width to train the classifier\n",
    "# Your code goes here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the coolest feature of Decision Tree is that we can visualise its decision making. Here we use the library graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please run this once to install the python wrapper of graphviz in order to display the Decision Tree\n",
    "#!pip3 install graphviz\n",
    "!conda install -y python-graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Source\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "export_graphviz(\n",
    "        tree_clf,\n",
    "        out_file=os.path.join(IMAGES_PATH, \"iris_tree.dot\"),\n",
    "        feature_names=iris.feature_names[2:],\n",
    "        class_names=iris.target_names,\n",
    "        rounded=True,\n",
    "        filled=True\n",
    "    )\n",
    "\n",
    "Source.from_file(os.path.join(IMAGES_PATH, \"iris_tree.dot\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this command to save the .dot file to an image\n",
    "!dot -Tpng images/decision_trees/iris_tree.dot -o iris_tree.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through one prediction done by DecisionTreeClassifier on iris dataset\n",
    "- First given a flower with petal length in cm. It will check whether this feature is less than 2.45cm.\n",
    "- If this is True, then it moves down the left node and here among 50 samples which has the petal length <= 2.45 cm, all of them are setosa.\n",
    "- If the petal length is greater than 2.45cm, then it moves down the right node. \n",
    "- On this 2nd level, it compares the petal width with 1.75cm. If it is less than 1.75cm, then it moves down the left childnode. Here, among 54 samples with petal width <= 1.75cm, 49 of them are versicolor, hence the prediction would be versicolor.\n",
    "- Otherwise if petal width is greater than 1.75 then out of 46 samples, 45 of them are virginica.\n",
    "\n",
    "Here are some of the important attributes within each node:\n",
    "- gini: measures the impurity of the training instances that it applies to. If gini is 0, that means all of the instances belong to one class (for example: setosa)\n",
    "- samples: the number of instances satisfies the feature condition.\n",
    "- values: the separation into classes that having the feature condition.\n",
    "- class: the current classification for the instance. We only take the class of the leaf node as the final classification.\n",
    "\n",
    "Scikit-learn uses CART (Classification And Regression Tree) algorithm to generate binary tree. Hence all nodes are having zero or 2 leaf nodes. \n",
    "You can control how deep you want to grow the trees (and Yes, in Computer Science you grow the tree downwards) by changing the `max_depth` parameter of DecisionTreeClassifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the plot_decision_boundary() function here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try out a few different configurations of the DecisionTreeClassifier()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise ####\n",
    "- Change random_state to see different boundary\n",
    "- Modify min_samples_leaf, min_samples_split\n",
    "- Change the depth of the tree and then visualise the decision tree as well as the decision boundary\n",
    "- Test the prediction in a flower with petals_length is 5 cm and petals_width is 1.5cm. What happened if we change the max_depth from 2 to 3?\n",
    "\n",
    "As you can see, DecisionTree is not very stable, changining random_state might lead to completely different decision boundary. We will see how RandomForests can be used to limit this instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use DecisionTree for Regression problem\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the Decision Tree Classifider on Titanic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "# We read out the processed dataset from Logistic Regression\n",
    "final_train = pd.read_csv('data/final_titanic_train.csv')\n",
    "\n",
    "# create X (features) and y (response)\n",
    "X_titanic = final_train.loc[:, ~final_train.columns.isin(['Survived'])]\n",
    "y_titanic = final_train['Survived']\n",
    "\n",
    "# use train/test split with different random_state values\n",
    "# we can change the random_state values that changes the accuracy scores\n",
    "# the scores change a lot, this is why testing scores is a high-variance estimate\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_titanic, y_titanic, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a simple (no configuration) decision tree classifier\n",
    "# TODO:\n",
    "# Your code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the score\n",
    "# TODO:\n",
    "# Your code goes here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross validation score to validate the accuracy of the training\n",
    "# TODO:\n",
    "# Your code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventhough it achieves 98% accuracy on the training data, its cross validation score is only ~79%, which means it overfitted the data.\n",
    "One way to regularise decision tree model is to limit the depth of the tree, we can try to do it manually as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cross validation on different values for max depth to determine the suitable value\n",
    "# TODO:\n",
    "# Your code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the tree pruning method, with a depth of 3 we are able to achieve 81.4% accuracy, which is encouraging given that we do not do much on manipulating the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros/ Cons of Decision Tree\n",
    "Advantages of Decision Tree:\n",
    "\n",
    "- Easy to explain and visualise the decision making\n",
    "- Highly interpretable.\n",
    "- Model training and prediction are fast.\n",
    "- Most of the time, it does not require feature scaling or centering\n",
    "\n",
    "Disadvantages of Decision Tree:\n",
    "- Instability with changes in data\n",
    "- CART is a greedy algorithm, therefore it is not guaranteed to perform optimally across different training sets or even among subsets of a training data.\n",
    "- It does not work well with unbalanced dataset, easily biased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise:\n",
    "Use graphviz to visualise the decision tree for the titanic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Your code goes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) DecisionTreeRegressor for regression tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
